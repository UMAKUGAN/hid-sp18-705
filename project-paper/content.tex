% status: 66
% chapter: TBD

\def\paperstatus{66} % a number from 0-100 indicating your status. 100
                % means completed
\def\paperchapter{TBD} % This section is typically a single keyword. from
                   % a small list. Consult with theinstructors about
                   % yours. They typically fill it out once your first
                   % text has been reviewed.
\def\hid{hid-sp18-705} % all hids of the authors of this
                                % paper. The paper must only be in one
                                % authors directory and all other
                                % authors contribute to it in that
                                % directory. That authors hid must be
                                % listed first
\def\volume{9} % the volume of the proceedings in which this paper is to
           % be included

\def\locator{\hid, Volume: \volume, Chapter: \paperchapter, 
	Status: \paperstatus. \newline}

\title{New Approaches to Managing Metadata at Scale in Research Libraries}
\author{Timothy A. Thompson}
\affiliation{%
  \institution{Indiana University Bloomington}
  \streetaddress{School of Informatics, Computing, and Engineering}
  \city{Bloomington} 
  \state{Indiana} 
  \postcode{47408}
}
\email{timathom@indiana.edu}

\begin{abstract} 
The analysis of big data often relies on distributed storage and
computation; however, access to big data---and to the platforms capable of
managing and processing it---continues to be largely centralized.
Centralization is particularly evident in the case of the metadata produced,
managed, and disseminated by academic and research libraries. Libraries
typically create and share their catalog records by uploading them to a
centrally managed database, which can then be searched by other libraries for
records that can be copied and added to an institution's local catalog. This
centralized approach, which operates on the basis of membership fees, has the
advantage of scalability and availability, but it comes at the cost of a loss
of autonomy. Although technical innovation is possible within the current
paradigm, the growing maturity of peer-to-peer protocols and decentralized
solutions points toward an alternative approach, one that would allow
libraries to share their data directly without having to pay an expensive
intermediary.
\end{abstract}

\keywords{i523, hid-sp18-705, Research Libraries, Library Catalogs, 
Peer-to-Peer, Blockchain}

\maketitle

\section{Introduction}
The problem of entity resolution (also known as record linkage or data
matching~\cite{pC12}) is one that has a direct impact on the work of
information professionals in research libraries. In library units
responsible for catalog management, many workflows center on a procedure
known as copy cataloging, which aims to expedite the processing of new
acquisitions. Copy cataloging involves searching a shared database for
records created by another cataloging agency, but that describe identical
publications that have been acquired by one's local institution~\cite{cD17}.
In the current environment, a single company, the Online Computer Library
Center (OCLC---\url{http://www.oclc.org}), is the only viable platform for
global cooperative cataloging~\cite{aT10}. OCLC provides data aggregation
and warehousing services that allow libraries to effectively share their
data, but its business model does not encourage peer-to-peer interaction and
innovation among individual libraries. This vendor-driven paradigm entails
the acceptance of a business model that, in effect, charges libraries for
serving their own data back to them, with some added value through quality
control and normalization. Once a library's data has been sent to OCLC, it
also becomes subject to potential licensing restrictions, as well as the
expectation that future dissemination of the data will include attribution
of OCLC~\cite{oclcND, oclc10}.

\section{New Approaches to Metadata Management}
Libraries have a tradition of experience with record matching and
automation~\cite{jM92}, but now stand to benefit from the increasingly
mainstream availability of algorithms and routines developed within the
context of data science and machine learning. Sophisticated algorithms for
string comparison and probabilistic data record linkage have long been
available, but are not widely used by libraries, with the exception of
large-scale projects such as the Social Networks and Archival Context
Project (SNAC) (\url{http://snaccooperative.org/}) and the Virtual
International Authority File (VIAF) (\url{http://viaf.org/}). The former has
employed methods based on Naive Bayes classification algorithms to aggregate
and disambiguate data from across a wide range of libraries and archives
(the reported accuracy of the approach fell with the range of 80-90
percent)~\cite{rL11}. More recent approaches to record matching have
improved on probabilistic methods such as Naive Bayes by using Artificial
Neural Networks, improving accuracy rates in some cases to 98 percent or
more~\cite{rG17}.

As machine learning tools and methods have become more accessible,
however, large-scale, real-time access to library metadata has not
necessarily followed suit. The catalog of a large academic library may
contain around 10 million records~\cite{yul18}. By comparison, as of August
2018, the OCLC catalog database, WorldCat, contained 427,501,671
bibliographic records in 491 languages. As long as service providers such as
OCLC maintain centralized control over the aggregated metadata of research
libraries, large-scale computational analysis---and the innovation it could
produce---will remain proprietary and locked away.

ï»¿New distributed technologies and protocols, including blockchains
and distributed hash tables (DHTs), could allow research libraries to form
robust peer-to-peer networks that would enable data sharing on a larger
scale. Although public blockchains such as Ethereum and Bitcoin are limited
in the amount of data that can feasibly be stored on chain, alternative
platforms that address this limitation have recently emerged. The
blockchain-based database service BigchainDB, implemented in Python,
provides a robust storage data solution while preserving the benefits of
blockchain features such as data immutability and an asset-based
transactional model. By running a consortial blockchain network of
BigchainDB nodes, libraries could be empowered to abandon centralized models
and begin managing their data collectively. 

Shown in Figure~\ref{f:rbac}. Shown in Figure~\ref{f:bdb1}.


\begin{figure}[!htb]
	\centering\includegraphics[width=\columnwidth]{images/rbac-graph.pdf}  
	\caption{Graph of permissions in BigchainDB using Role-Based Access 
	Control}\label{f:rbac}
\end{figure}

\begin{figure}[!htb]
	\centering\includegraphics[width=\columnwidth]{images/bdb-seq.pdf}  
	\caption{BigchainDB Sequence Diagram~\cite{aA17}}\label{f:bdb1}
\end{figure}

\begin{figure}[!htb]
	\centering\includegraphics[width=\columnwidth]{images/bdb-arch.pdf}  
	\caption{BigchainDB Architecture Diagram~\cite{aA17}}\label{f:bdb2}
\end{figure}

\section{Conclusion}


\begin{acks}
The author would like to thank Dr.~Gregor~von~Laszewski and the i523
teaching assistants for their support and suggestions in writing this
report.
\end{acks}

\bibliographystyle{ACM-Reference-Format}
\bibliography{report} 

